Here you will find answers to the most frequently asked questions. Please also refer to the [issues](https://github.com/microsoft/bc2adls/issues/) site to know more or to ask your own questions in the community. 

### How do I run the export to the lake in a recurring schedule?
The [Job Queue](https://learn.microsoft.com/en-us/dynamics365/business-central/admin-job-queues-schedule-tasks) feature in Business Central is used to schedule background tasks in a periodic way. You may invoke the [Codeunit `ADLSE Execution`](/businessCentral/src/ADLSEExecution.Codeunit.al) through the feature to export the data increments to the lake as a scheduled job. You may click `Schedule export` on the main setup page to create this job queue.

### I have many BC environments in the same tenant. How should I distribute the BC data to my lake?
We recommend that a data lake container holds data only for **only one** Business Central environment. After copying environments, ensure that the export destination on the setup page on the new environment points to a new data lake container.

### How do I export data from multiple companies in the same environment?
The export process copies the updated data to the lake for ONLY the company it has been invoked from. This is true whether you start the process by a click on the `Export` button or by scheduling a `Job Queue Entry`. Therefore, one should log in and click the button or setup scheduled jobs from the company whose data needs to be exported. A field called `Multi- company export` was added in [Pull Request #47](https://github.com/microsoft/bc2adls/pull/47) to improve concurrency for parallel exports from different companies. The field, in and of itself, does not export data from  other companies.

### Can I export calculated fields into the lake?
No, only persistent fields on the BC tables can be exported. But, the [issue #88](https://github.com/microsoft/bc2adls/issues/88) describes a way to show up those fields when consuming the lake data.

### How can I export BLOB data to the lake?
Data from blob fields in tables are not exported today to the lake. It should be possible however to convert the (possibly, binary) data to text using the [Codeunit `Base64 Convert`](https://learn.microsoft.com/en-us/dynamics365/business-central/application/reference/system%20application/codeunit/system_application_codeunit_base64_convert) and then store it as a separate field in a new table and exporting it to the lake using the bc2adls solution.

### How do I export some tables at a different frequency than the rest?
Normally, all the tables that are setup for export will export at the same time. However you may invoke exports of selected tables by using the API pages available. The [issue #87](https://github.com/microsoft/bc2adls/issues/87) describes this possibility in more detail.

### How do I track the files in the `deltas` folder in my data lake container?
Incremental exports create files in the `deltas` folder in the lake container. Each such file has a `Modified` field that indicates the time when it was last updated, in other words, when the export process finished with that file. Each export process for an entity and in a company logs its execution on the  [`ADLSE Run`](/businessCentral/src/ADLSERun.Table.al) table using the `Started` and `Ended` fields. Thus you may tally the value in the `Modified` field of the file to these fields and determine which run resulted in creation of that file. You may also use telemetry to determine which run created which file.

### What should I do when a field I was exporting has been made obsolete? How do I handle schema changes in my data?
An upgrade may alter the set of fields in a table. **Schema changes are not supported in `bc2adls` and the recommended path is to archive the "old" data for such entities and simply re-export all data with the new schema to the lake after re-setting the tables from the setup page.** A schema change often appears together with an upgrade logic that may populate alternate fields (possibly, in different entities). This makes it really hard to create a "generic" way to handle schema changes inside `bc2adls`. 

Table fields that are obsoleted already cannot be configured to be exported but the system actually allows you to add fields that are pending obsoletion. In case you are already using such a field, you will get upgrade errors when upgrading to newer versions of the application where the field has been removed. It is recommended that you read the documentation of the obsoletion (also check the upgrade code to handle this change) to determine if there are different fields that will hold the information from the new version onwards and then to enable those fields thereby. Of course, you will also have to disable the obsoleted field from the export. 

Having said that, here is a suggestion that could be adopted by those who feel comfortable with both BC as well as Azure Synapse. **This approach has inherent risks of corrupting the CDM folder and making the data lake inaccessible. Therefore, please exercise caution and attempt it only if you are confident of possessing the right skills.**
1. First, [consolidate all the files](/.assets/Execution.md#running-the-integration-pipeline) in the `deltas` folder of the  entities whose schemas have changed (either fields have been added to or obsoleted from them).
1. Archive the `data` files in the lake for the corresponding entities, perhaps, to a different container. Remove them from your main container. Copy the old jsons as well so that the new container is still a fully functional CDM folder.
1. Now, go to BC and visit the setup page and ensure that the export schema includes the new fields and excludes the old ones. Export data only from the necessary entities (by disabling other entities), making sure that the `Multi- company export` field is unchecked. This should result in new entity and manifest jsons on the data lake.
1. In order to bring the archived data into the data folder, create a new Synapse pipeline calling a dataflow that,
    * reads the old CDM data from the archive container as source
    * if a new field has replaced the obsolete field in BC, adds the new field with the calculation of the new value derived from looking at the upgrade logic for the change
    * writes the data into the CDM entity in the main container as the sink, having mapped all the relevant columns manually 

Read more [here](https://learn.microsoft.com/en-us/azure/data-factory/format-common-data-model) or take a look at the [`Consolidation_flow`](/synapse/dataflow/Consolidation_flow.json) in your Synapse workspace to understand how to read/ write CDM folders in your Synapse data flows.

### I need help because my export job is timing out!
Let's look at addressing timeout issues that have been seen to occur at two possible places in the solution, both of them happening typically during the initial export of records,  
1. The query to fetch the records during before the export to the lake may timeout if it takes more than the [operation limits](https://learn.microsoft.com/en-us/dynamics365/business-central/dev-itpro/administration/operational-limits-online) defined. This may happen when bc2adls attempts to sort a large set of records as per the row version. You may _suspend_ the sorting temporarily using the field `Skip row version sorting` on the setup page. 
1. Chunks of data (or [blocks](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction#:~:text=Block%20blobs), in the data lake parlance) are added to a lake file during the export. Adding too many such large chunks may cause timeout issues in the form of an error message like `Could not commit blocks to <redacted>. OperationTimedOutOperation could not be completed within the specified time.` We are using default timeouts in the bc2adls app, but you may add [additional timeout URL parameter](https://learn.microsoft.com/en-us/rest/api/storageservices/put-block-list?tabs=azure-ad#:~:text=timeout) if you want by suffixing the URL call in the procedure [`CommitAllBlocksOnDataBlob`](/businessCentral/src/ADLSEGen2Util.Codeunit.al#:~:text=CommitAllBlocksOnDataBlob) with `?timeout=XX`, XX being the number of seconds for timeout to expire. This issue could typically happen when you are pushing a large payload to the server. Also consider reducing the number at the field [Max payload size (MiBs)](/.assets/Setup.md#:~:text=Max%20payload%20size%20(MiBs)).